{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cltk.corpus.readers import get_corpus_reader\n",
    "latin_corpus = get_corpus_reader(corpus_name = 'latin_text_latin_library', language = 'latin')\n",
    "from cltk.stem.lemma import LemmaReplacer\n",
    "lemmatizer = LemmaReplacer('latin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2141"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(latin_corpus.docs()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = latin_corpus.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "livy_path = [file for file in files if 'livy' in file and 'per' not in file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['livy/liv.1.txt', 'livy/liv.10.txt', 'livy/liv.2.txt', 'livy/liv.21.txt', 'livy/liv.22.txt', 'livy/liv.23.txt', 'livy/liv.24.txt', 'livy/liv.25.txt', 'livy/liv.26.txt', 'livy/liv.27.txt', 'livy/liv.28.txt', 'livy/liv.29.txt', 'livy/liv.3.txt', 'livy/liv.30.txt', 'livy/liv.31.txt', 'livy/liv.32.txt', 'livy/liv.33.txt', 'livy/liv.34.txt', 'livy/liv.35.txt', 'livy/liv.36.txt', 'livy/liv.37.txt', 'livy/liv.38.txt', 'livy/liv.39.txt', 'livy/liv.4.txt', 'livy/liv.40.txt', 'livy/liv.41.txt', 'livy/liv.42.txt', 'livy/liv.43.txt', 'livy/liv.44.txt', 'livy/liv.45.txt', 'livy/liv.5.txt', 'livy/liv.6.txt', 'livy/liv.7.txt', 'livy/liv.8.txt', 'livy/liv.9.txt', 'livy/liv.pr.txt']\n"
     ]
    }
   ],
   "source": [
    "print(livy_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "livy_paras = latin_corpus.paras(livy_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "livy_paras_list = list(livy_paras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "livy_words = latin_corpus.words(livy_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "livy_words_list = list(livy_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Livy', ':', 'Book', 'I', 'TITI', 'LIVI', 'AB', 'VRBE', 'CONDITA', 'LIBER']\n"
     ]
    }
   ],
   "source": [
    "print(livy_words_list[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is for the tf-idf; I thought maybe I'd need to have the whole document?\n",
    "from cltk.stop.latin import STOPS_LIST\n",
    "from cltk.stem.lemma import LemmaReplacer\n",
    "\n",
    "low_livy_list = [word.lower() for word in livy_words_list]\n",
    "lemmatizer = LemmaReplacer('latin')\n",
    "lematize_livy = [lemmatizer.lemmatize(word) for word in low_livy_list]\n",
    "S = STOPS_LIST\n",
    "flat_list = [item for sublist in lematize_livy for item in sublist]\n",
    "livy_stops_removed = [w for w in flat_list if w not in STOPS_LIST]\n",
    "junk = ['cn.', 't.', 'q.', \"'\", 'm.', 'p.', '[', ']', '.', ',', ' ', ':', ';', 'qui1', '-', 'que', '$', '%', '&','*','+', '-', '/', '<', '=', '>', '@', '^', '_',  '`', '{', '|', '}', '~', '?', '!', '«', '»']\n",
    "livy_junk_removed = [w for w in livy_stops_removed if w not in junk]\n",
    "clean_livy_words = livy_junk_removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['livy', 'book', 'eo1', 'titi', 'livi', 'vrbe', 'condio', 'libo1', 'eo1', '1']\n"
     ]
    }
   ],
   "source": [
    "print(clean_livy_words[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print(livy_paras_list[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2209"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(livy_paras_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#paras were wrapped in a list so need to go down one\n",
    "livy_paras_flat = [item[0] for item in livy_paras_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2209"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(livy_paras_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(livy_paras_flat[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2209"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(livy_paras_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now lower case everything\n",
    "lower_livy_list = [[w.lower() for w in paras] for paras in livy_paras_flat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cltk.stem.lemma import LemmaReplacer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = LemmaReplacer('latin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lematize paragraphs\n",
    "lematize_livy = [[lemmatizer.lemmatize(word, return_string = True) for word in paras] for paras in lower_livy_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove junk\n",
    "junk = ['.', ',', ', ', ' ', ':', ';', 'qui1', '-', 'que', '$', '%', '&','*','+', '-', '/', '<', '=', '>', '@', '^', '_',  '`', '{', '|', '}', '~', '?', '!', '«', '»']\n",
    "junk_removed = [[w for w in paras if w not in junk] for paras in lematize_livy]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove paragraphs that are only digits; rather return paragraphs that aren't only digits.\n",
    "no_numbers = [[w for w in paras if w.isdigit() == False] for paras in junk_removed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(no_numbers[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove paragraphs that are shorter than 5 words\n",
    "no_shorties = [para for para in no_numbers if len(para) >= 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2120"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(no_shorties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(no_shorties[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54574"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"check how many words in the document - this doesn't seem right? But could have something to do with processing\n",
    "paragraphs and lists?\n",
    "\"\"\"\n",
    "\n",
    "length_para = [len(para) for para in no_shorties]\n",
    "#print(length_para)\n",
    "sum(length_para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# give the finished product a better name\n",
    "clean_livy = no_shorties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(clean_livy[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    }
   ],
   "source": [
    "count = 0 \n",
    "for para in clean_livy:\n",
    "    for word in para:\n",
    "        if word == \"praeda\":\n",
    "            count += 1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is the function which may or may not be working. I doubt that there are only 30 paras with praeda for example ^^^\n",
    "def includes(*keyword):\n",
    "    count = 0\n",
    "    for para in clean_livy:\n",
    "        #for para in item:\n",
    "            #if keyword in para:\n",
    "            #if any([keyword in para]) == True:\n",
    "        if any(word in para for word in keyword):\n",
    "            count += 1\n",
    "            \n",
    "    print(str(count) + \" out of \" + str(len(no_shorties)) + \" paragraphs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "902 out of 2120 paragraphs\n"
     ]
    }
   ],
   "source": [
    "includes(\"praeda\",\"consul\",\"sum1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checkpoint! Passed here is just tf-idf stuff which doesn't work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy\n",
    "import matplotlib\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "mytext = nltk.Text(clean_livy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from gensim import corpora\n",
    "from collections import defaultdict\n",
    "from gensim.corpora import Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(mytext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = [[word for word in para] for para in mytext]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "decoding to str: need a bytes-like object, list found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-cbed3e8ce905>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmydict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpara\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpara\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmytext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmydict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc2bow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpara\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpara\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmytext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/Code/cltk/venv/lib/python3.6/site-packages/gensim/corpora/dictionary.py\u001b[0m in \u001b[0;36mdoc2bow\u001b[0;34m(self, document, allow_update, return_missing)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0mcounter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocument\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0mcounter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0municode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0mtoken2id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken2id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: decoding to str: need a bytes-like object, list found"
     ]
    }
   ],
   "source": [
    "mydict = corpora.Dictionary([para for para in mytext])\n",
    "corpus = [mydict.doc2bow(para for para in mytext)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
