{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### import dependencies \n",
    "\n",
    "import re\n",
    "\n",
    "from cltk.corpus.readers import get_corpus_reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get corpus\n",
    "\n",
    "latin_corpus = get_corpus_reader(corpus_name = 'latin_text_latin_library', language = 'latin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FilteredPlaintextCorpusReader in '/Users/diyclassics/cltk_data/latin/text/latin_text_latin_library'>\n"
     ]
    }
   ],
   "source": [
    "### check file path\n",
    "print(latin_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2141"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### check how many items in corpus\n",
    "\n",
    "# PJB: Use fileids to get this number faster\n",
    "\n",
    "len(list(latin_corpus.fileids()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['12tables.txt', '1644.txt', 'abbofloracensis.txt', 'abelard/dialogus.txt', 'abelard/epistola.txt', 'abelard/historia.txt', 'addison/barometri.txt', 'addison/burnett.txt', 'addison/hannes.txt', 'addison/machinae.txt']\n"
     ]
    }
   ],
   "source": [
    "### show first 10 file ids that you can use in the text reader\n",
    "print(latin_corpus.fileids()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### set a variable that is the list of all file names so we can iterate over it\n",
    "files = latin_corpus.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### iterate over the files list and return only files which contain Livy and don't contain per\n",
    "\n",
    "# PJB: livy_files would be more accurate\n",
    "\n",
    "livy_files = [file for file in files if 'livy' in file and 'per' not in file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['livy/liv.pr.txt', 'livy/liv.1.txt', 'livy/liv.2.txt', 'livy/liv.3.txt', 'livy/liv.4.txt', 'livy/liv.5.txt', 'livy/liv.6.txt', 'livy/liv.7.txt', 'livy/liv.8.txt', 'livy/liv.9.txt', 'livy/liv.10.txt', 'livy/liv.21.txt', 'livy/liv.22.txt', 'livy/liv.23.txt', 'livy/liv.24.txt', 'livy/liv.25.txt', 'livy/liv.26.txt', 'livy/liv.27.txt', 'livy/liv.28.txt', 'livy/liv.29.txt', 'livy/liv.30.txt', 'livy/liv.31.txt', 'livy/liv.32.txt', 'livy/liv.33.txt', 'livy/liv.34.txt', 'livy/liv.35.txt', 'livy/liv.36.txt', 'livy/liv.37.txt', 'livy/liv.38.txt', 'livy/liv.39.txt', 'livy/liv.40.txt', 'livy/liv.41.txt', 'livy/liv.42.txt', 'livy/liv.43.txt', 'livy/liv.44.txt', 'livy/liv.45.txt']\n"
     ]
    }
   ],
   "source": [
    "# livy_path_sorted = sorted(livy_path)\n",
    "# print(livy_path_sorted)\n",
    "\n",
    "# PJB: See below for a book sort; with a workaround here for the Preface\n",
    "\n",
    "livy_files.remove('livy/liv.pr.txt')\n",
    "livy_files_order = [int(\" \".join(re.findall(r'\\d+', item))) for item in livy_files]\n",
    "livy_files_sorted = ['livy/liv.pr.txt']\n",
    "livy_files_sorted += [x for _, x in sorted(zip(livy_files_order, livy_files))]\n",
    "print(livy_files_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### check that we're getting what we want; yep Livy books 1-45 n.b 11-20 don't exist so won't show up plus 1 is .\n",
    "#print(livy_path_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check that 36 texts show up i.e. books 1-10;21-45 and the preface\n",
    "len(livy_files_sorted) == 36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object FilteredPlaintextCorpusReader.words at 0x1121f0d68>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#use the .words method from the cltk corpus reader function to read the texts in Livy\n",
    "#and return each word in each text\n",
    "livy_words = latin_corpus.words(livy_files)\n",
    "livy_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### convert the generator object into a list of words\n",
    "livy_words_list = list(livy_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show the first 100 words so we can get an idea for the data we have\n",
    "# print(livy_words_list[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the words into lower case\n",
    "low_livy_list = [word.lower() for word in livy_words_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the result\n",
    "#print(low_livy_list[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import lematizer\n",
    "\n",
    "# PJB: Deprecated; different code below; make sure to reload the latin models corpus\n",
    "# from cltk.corpus.utils.importer import CorpusImporter\n",
    "# corpus_importer.import_corpus('latin_models_cltk')\n",
    "\n",
    "# from cltk.stem.lemma import LemmaReplacer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # tell the lematizer it will be the latin version as opposed to e.g. Ancient Greek\n",
    "# lemmatizer = LemmaReplacer('latin')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iterate over list of words and lemamatize each\n",
    "# lematize_livy = [lemmatizer.lemmatize(word) for word in low_livy_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(lematize_livy[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('livy', 'livy'), (':', 'punc'), ('book', 'book'), ('i', 'eo'), ('titi', 'titus'), ('livi', 'livi'), ('ab', 'ab'), ('vrbe', 'vrbe'), ('condita', 'condio'), ('liber', 'liber')]\n"
     ]
    }
   ],
   "source": [
    "# New lemmatizer code\n",
    "\n",
    "from cltk.lemmatize.latin.backoff import BackoffLatinLemmatizer\n",
    "lemmatizer = BackoffLatinLemmatizer()\n",
    "lemmas = lemmatizer.lemmatize(low_livy_list)\n",
    "print(lemmas[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "still working, checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cltk's pre-made stopword list\n",
    "from cltk.stop.latin import STOPS_LIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = STOPS_LIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check what they have in their list\n",
    "#print(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when we lematized the list it wrapped it in a second list so we have to get back to just one list\n",
    "flat_list = [item for sublist in lemmas for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that flat list\n",
    "#print(flat_list[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "livy_stops_removed = [w for w in flat_list if w not in STOPS_LIST]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare my own list of junk words or symbols not caught by cltk\n",
    "junk = ['cn.', 't.', 'q.', \"'\", 'm.', 'p.', '[', ']', '.', ',', ' ', ':', ';', 'qui1', '-', 'que', '$', '%', '&','*','+', '-', '/', '<', '=', '>', '@', '^', '_',  '`', '{', '|', '}', '~', '?', '!', '«', '»']\n",
    "livy_junk_removed = [w for w in livy_stops_removed if w not in junk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "947872"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#compare the number of words before and after the junk is removed\n",
    "len(livy_stops_removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "866395"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(livy_junk_removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-declare the variable so it makes more sense\n",
    "clean_livy = livy_junk_removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this base python method will help us create a dictionary of word frequencies\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the counter to our cleaned text\n",
    "livy_word_counts_counter = Counter(clean_livy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "# it works, just put whatever word you want in and it will return its word count\n",
    "print(livy_word_counts_counter['manubiae'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "^^^ checkpoint: lematized, cleaned, counter working. ^^^"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-6152e34a9da5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprobability\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFreqDist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy\n",
    "import matplotlib\n",
    "from nltk.probability import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now to interact with the more interesting natural language processing libraries we need to convert our processed text\n",
    "# into nltk tokens.\n",
    "mytext = nltk.Text(clean_livy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mytext[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the nltk dispersion plot function to show how words we are interested in are spread across a text\n",
    "# n.b must run TWICE before it works\n",
    "mytext.dispersion_plot([\"praeda\", \"manubiae\", \"aurum\", \"argentum\", \"stipendium\", \"tribuo\", \"bos\", \"ager\", \"consul\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist1 = FreqDist(mytext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fdist1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's get a list of word counts throughout Livy\n",
    "# praeda is in the top 250 nouns or verbs list\n",
    "# we could further refine this to work with only nouns\n",
    "fdist1.most_common(250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's look at a graph of word occurences\n",
    "fdist1.plot(50, cumulative=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_bigrams = list(nltk.bigrams(mytext))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(my_bigrams[100:110])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_bigrams(keyword):\n",
    "    for tuple in my_bigrams:\n",
    "        if keyword in tuple:\n",
    "            print(tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_bigrams(\"praeda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are now going to build a netword of words in Livy using the co-occurence method\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a network whereby each word is connected by an edge to the words either side of it\n",
    "G=nx.Graph()\n",
    "for i in range(1, len(mytext)):\n",
    "    G.add_edge(mytext[i-1],mytext[i])\n",
    "print(nx.info(G))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many connections does each word have?\n",
    "degree = nx.degree(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_degree = sorted(dict(nx.degree(G)).items(),key=lambda x:x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show the 250 words with most connections\n",
    "#these ranks will be similar to our word frequency score\n",
    "for word, degree in sorted_degree[:250]:\n",
    "        print(word, degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we look for degree centrality. That, similarly, is a score of how close to the middle of a network a node is.\n",
    "sorted_degree_centrality = sorted(nx.degree_centrality(G).items(),key=lambda x:x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_degree_centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# betweeness is more interesting though. It finds the shortest path between each node on a network and then tells us\n",
    "# how frequently a node is on that shortest path\n",
    "# N.b. this is a VERY long computation so don't run it unless you really want to know the betweeness centrality!\n",
    "#sorted_betweeness = sorted(nx.betweenness_centrality(G).items(),key=lambda x:x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_betweeness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save our network to a file that other applications can undertsand.\n",
    "nx.write_gexf(G, \"livy_network.gexf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
